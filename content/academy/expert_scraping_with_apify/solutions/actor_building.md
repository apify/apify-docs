---
title: I - Actor building
description: Build a pro Apify actor. With all of the code provided right within the lesson, you can learn advanced scraping techniques by coding along!
menuWeight: 1
paths:
    - expert-scraping-with-apify/solutions/actor-building
---

# [](#actor-building) Actor building

The Apify CLI makes it extremely easy for us to set up a project in the SDK and hit the ground running. Navigate to the directory you'd like your project's folder to live, then open up a terminal instance and run the following command:

```shell
apify create demo-actor
```

> You don't have to call it **demo-actor**, but that's what we'll be calling it within this tutorial.

Once you run this command, you'll get prompted into a menu which you can navigate using your arrow keys. Each of these options will generate different boilerplate code when selected. We're going to work with CheerioCrawler today, so we'll select that one, then press **Enter**.

![Apify CLI "create" command]({{@asset expert_scraping_with_apify/solutions/images/apify-create.webp}})

Once it's completed, open up the **demo-actor** folder that was generated by the `apify create` command. We're going to modify the boilerplate to fit our needs:

```JavaScript
// main.js
const Apify = require('apify');

const { log } = Apify.utils;

Apify.main(async () => {
    // Grab our keyword from the input
    const { keyword } = await Apify.getInput();

    // Open a request list for our initial requests
    const requestList = await Apify.openRequestList('start-urls', [
        {
            // Turn the inputted keyword into a link we can make a request with
            url: `https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,
            userData: {
                label: 'START',
                keyword,
            },
        },
    ]);

    // Open a request queue for our future requests
    const requestQueue = await Apify.openRequestQueue();

    // Use the RESIDENTIAL proxy group to prevent blocking
    const proxyConfiguration = await Apify.createProxyConfiguration({
        groups: ['RESIDENTIAL'],
    });

    const crawler = new Apify.CheerioCrawler({
        requestList,
        requestQueue,
        proxyConfiguration,
        useSessionPool: true,
        maxConcurrency: 50,
        handlePageFunction: async ({ $ }) => {
            // our future code will go here
        },
    });

    log.info('Starting the crawl.');
    await crawler.run();
    log.info('Crawl finished.');
});
```

Finally, we'll modify our input file in **apify_storage/key_value_stores/default/INPUT.json** to look like this:

```JSON
{
  "keyword": "iphone"
}
```

Cool! We're ready. Let's go!

## [](#lets-do-it) Let's scrape Amazon! ðŸ’ª

Now that we've gotten our first request going, the first challenge is going to be selecting all of the resulting products on the page. Back in the browser, we'll use the DevTools hover tool to inspect a product.

![Result products]({{@asset expert_scraping_with_apify/solutions/images/result-items.webp}})

**Bingo!** Each product seems to have a `data-asin` attribute, which includes the ASIN (product ID) data we want. Now, we can select each of these elements with this selector: `div > div[data-asin]:not([data-asin=""])`. Then, we'll scrape some data about each product, and push a request to the main product page so we can grab hold of the description.

## [](#modularity) Quick break to chat about modularity

But, before we start scraping, let's pause to talk a bit about the important concept of **modularity**. You may have noticed the **src** folder inside of your project, which by default has a **routes.js** file in it. We're going to use this to create modularized functions which we can then import and call in our main file.

```JavaScript
// src/routes.js
const { BASE_URL } = require('./constants');

// export our function, which expects the "context" object to be passed to it
exports.handleStart = async ({ $, crawler: { requestQueue }, request }) => {
    const { keyword } = request.userData;

    const products = $('div > div[data-asin]:not([data-asin=""])');

    // loop through the resulting products
    for (const product of products) {
        const element = $(product);
        const titleElement = $(element.find('.a-text-normal[href]'));

        const url = `${BASE_URL}${titleElement.attr('href')}`;

        // scrape some data from each and to a request
        // to the request list for its page
        await requestQueue.addRequest({
            url,
            userData: {
                label: labels.PRODUCT,
                // Pass the scraped data about the product to the next
                // request so that it can be used there
                data: {
                    title: titleElement.first().text().trim(),
                    asin: element.attr('data-asin'),
                    itemUrl: url,
                    keyword,
                },
            },
        });
    }
};
```

Also notice that we are importing `BASE_URL` from **constants.js**. Here is what that file looks like:

```JavaScript
const BASE_URL = 'https://www.amazon.com';

module.exports = { BASE_URL };
```

And here is what our **main.js** file currently looks like:

```JavaScript
// main.js
const Apify = require('apify');

// Import the function here
const { handleStart } = require('./src/routes');
const { BASE_URL } = require('./src/constants');

const { log } = Apify.utils;

Apify.main(async () => {
    const { keyword } = await Apify.getInput();

    const requestList = await Apify.openRequestList('start-urls', [
        {
            url: `${BASE_URL}/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,
            userData: {
                label: 'START',
                keyword,
            },
        },
    ]);

    const requestQueue = await Apify.openRequestQueue();
    const proxyConfiguration = await Apify.createProxyConfiguration({
        groups: ['RESIDENTIAL'],
    });

    const crawler = new Apify.CheerioCrawler({
        requestList,
        requestQueue,
        proxyConfiguration,
        useSessionPool: true,
        maxConcurrency: 50,
        handlePageFunction: async (context) => {
            const { label } = context.request.userData;

            switch (label) {
                default:
                    return log.info('Unable to handle this request');
                case 'START': {
                    // Use our imported function here
                    await handleStart(context);
                    break;
                }
                case 'PRODUCT': {
                    console.log('on a product page!');
                }
            }
        },
    });

    log.info('Starting the crawl.');
    await crawler.run();
    log.info('Crawl finished.');
});
```

One of the main reasons we **modularize** our code is to prevent massive and difficult to read files. Organized code makes everyone happy, including you - the one developing the solution! Spaghetti is super good, [but not when it comes to programming](https://www.urbandictionary.com/define.php?term=spaghetti+code) ðŸ

This can even be optimized further by putting our `label` items into **constants.js**, like so:

```JavaScript
const BASE_URL = 'https://www.amazon.com';

const labels = {
    START: 'START',
    PRODUCT: 'PRODUCT',
    OFFERS: 'OFFERS',
};

module.exports = { BASE_URL, labels };
```

Then, the labels can be used by importing `labels` and doing `labels.START`, `labels.PRODUCT`, or `labels.OFFERS`.

This is not necessary, but it is best practice, as it can prevent dumb typos that can cause nasty bugs ðŸž For the rest of this lesson, all of the examples using labels will be using the imported versions.

> If you haven't already read the **Best practices** lesson in the **Web scraping for beginners** course, please [give it a read]({{@link web_scraping_for_beginners/best_practices.md}}).

## [](#continue) Let's continue

In our quick chat about modularity, we finished the code for the results page, and added a request to the `requestQueue` for each product. Here, we just need to scrape the description, so it shouldn't be too hard:

```JavaScript
exports.handleProduct = async ({ $, crawler: { requestQueue }, request }) => {
    const { data } = request.userData;

    const element = $('div#productDescription');

    const description = element.text().trim();

    console.log(description); // works!
};
```

> If you are sometimes getting an error along the lines of **RequestError: Proxy responded with 407**, don't worry, this is totally normal. The proxy being used for the request was probably dead, so the request failed, but CheerioCrawler will automatically retry the request.

Great! But wait, where do we go from here? We need to go to the offers page next and scrape each offer, but how can we do that? Let's take a small break from writing the scraper and open up [Proxyman]({{@link tools/proxyman.md}}) to analyze requests which we can't see inside the network tab, then we'll click the button on the product page that loads up all of the product offers:

![View offers button]({{@asset expert_scraping_with_apify/solutions/images/view-offers-button.webp}})

After clicking this button and checking back in Proxyman, we discovered this link:

```text
https://www.amazon.com/gp/aod/ajax/ref=auto_load_aod?asin=B07ZPKBL9V&pc=dp
```

The `asin` parameter matches up with our product's ASIN, which means we can use this for any product of which we have the ASIN.

Here's what this page looks like:

![View offers page]({{@asset expert_scraping_with_apify/solutions/images/offers-page.webp}})

Wow, that's ugly. But for our scenario, this is really great. When we click the **View offers** button, we usually have to wait for the offers to load and render, which would mean we could have to switch our entire crawler to a **PuppeteerCrawler** or **PlaywrightCrawler**. The data on this page we've just found appears to be loaded statically, which means we can still use CheerioCrawler and keep the scraper as efficient as possible ðŸ˜Ž

First, we'll create a function which can generate an offers URL for us in **constants.js**:

```JavaScript
const OFFERS_URL = (asin) => `${BASE_URL}/gp/aod/ajax/ref=auto_load_aod?asin=${asin}&pc=dp`;
```

Then, we'll import and use that function to create a request for each product's offers page:

```JavaScript
exports.handleProduct = async ({ $, crawler: { requestQueue }, request }) => {
    const { data } = request.userData;

    const element = $('div#productDescription');

    // Add to the request queue
    await requestQueue.addRequest({
        url: OFFERS_URL(data.asin),
        userData: {
            label: labels.OFFERS,
            data: {
                ...data,
                description: element.text().trim(),
            },
        },
    });
};
```

## [](#final-code) Final code

That should be it! Let's just make sure we've all got the same code:

```JavaScript
// src/constants.js
const BASE_URL = 'https://www.amazon.com';

const OFFERS_URL = (asin) => `${BASE_URL}/gp/aod/ajax/ref=auto_load_aod?asin=${asin}&pc=dp`;

const labels = {
    START: 'START',
    PRODUCT: 'PRODUCT',
    OFFERS: 'OFFERS',
};

module.exports = { BASE_URL, OFFERS_URL, labels };
```

```JavaScript
// main.js
const Apify = require('apify');

const { handleStart, handleProduct, handleOffers } = require('./src/routes');
const { BASE_URL, labels } = require('./src/constants');

const { log } = Apify.utils;

Apify.main(async () => {
    const { keyword } = await Apify.getInput();

    const requestList = await Apify.openRequestList('start-urls', [
        {
            url: `${BASE_URL}/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,
            userData: {
                label: 'START',
                keyword,
            },
        },
    ]);

    const requestQueue = await Apify.openRequestQueue();
    const proxyConfiguration = await Apify.createProxyConfiguration({
        groups: ['RESIDENTIAL'],
    });

    const crawler = new Apify.CheerioCrawler({
        requestList,
        requestQueue,
        proxyConfiguration,
        useSessionPool: true,
        maxConcurrency: 50,
        handlePageFunction: async (context) => {
            const { label } = context.request.userData;

            switch (label) {
                default:
                    return log.info('Unable to handle this request');
                case labels.START:
                    await handleStart(context);
                    break;
                case labels.PRODUCT:
                    await handleProduct(context);
                    break;
                case labels.OFFERS:
                    await handleOffers(context);
                    break;
            }
        },
    });

    log.info('Starting the crawl.');
    await crawler.run();
    log.info('Crawl finished.');
});
```

```JavaScript
// src/routes.js
const Apify = require('apify');
const { BASE_URL, OFFERS_URL, labels } = require('./constants');

exports.handleStart = async ({ $, crawler: { requestQueue }, request }) => {
    const { keyword } = request.userData;

    const products = $('div > div[data-asin]:not([data-asin=""])');

    for (const product of products) {
        const element = $(product);
        const titleElement = $(element.find('.a-text-normal[href]'));

        const url = `${BASE_URL}${titleElement.attr('href')}`;

        await requestQueue.addRequest({
            url,
            userData: {
                label: labels.PRODUCT,
                data: {
                    title: titleElement.first().text().trim(),
                    asin: element.attr('data-asin'),
                    itemUrl: url,
                    keyword,
                },
            },
        });
    }
};

exports.handleProduct = async ({ $, crawler: { requestQueue }, request }) => {
    const { data } = request.userData;

    const element = $('div#productDescription');

    await requestQueue.addRequest({
        url: OFFERS_URL(data.asin),
        userData: {
            label: labels.OFFERS,
            data: {
                ...data,
                description: element.text().trim(),
            },
        },
    });
};

exports.handleOffers = async ({ $, request }) => {
    const { data } = request.userData;

    for (const offer of $('#aod-offer')) {
        const element = $(offer);

        await Apify.pushData({
            ...data,
            sellerName: element.find('div[id*="soldBy"] a[aria-label]').text().trim(),
            offer: element.find('.a-price .a-offscreen').text().trim(),
        })

    }
};
```

If Amazon hasn't changed any of their selectors or significantly updated any of their [anti-scraping measures]({{@link anti_scraping.md}}), the results in your dataset should each look like this:

```JSON
{
  "title": "Apple iPhone XR, 64GB, Black - Unlocked (Renewed)",
  "asin": "B07P6Y7954",
  "itemUrl": "https://www.amazon.com/Apple-iPhone-XR-Fully-Unlocked/dp/B07P6Y7954/ref=sr_1_3?keywords=iphone&qid=1649677535&sr=8-3",
  "keyword": "iphone",
  "description": "With the iPhone XR you get a roomy 6.1-inch display, fast enough performance from Apple's A12 Bionic processor and good camera quality in a colorful design and affordable package. Apple has included the all-new Liquid Retina LCD as the display on the iPhone XR.Â Apple released the iPhone XR with a smattering of color options. Both the glass back and the metal frame are brightly colored, with the glass using an in-depth seven-layer color process to achieve the rich finish and the Apple-exclusive aluminum alloy anodized to match. Instead of 3D Touch, the iPhone XR replicates the experience through \"Haptic Touch\". Advanced Face ID lets you securely unlock your iPhone and log in to apps with just a glance.",
  "sellerName": "Cell Surfers",
  "offer": "$249.97"
}
```

## [](#calling-another-actor) Calling another actor

If you remember from our project's requirements outlined in the previous lesson, once the crawler has finished running, we have to email ourselves a public link to the dataset by using a [public actor which sends emails](https://console.apify.com/actors/e643gqfZae2TfQEbA/?addFromActorId=e643gqfZae2TfQEbA#/console). Luckily, the ability to do this programmatically is avaialable right within the Apify SDK with the [`Apify.call()`](https://sdk.apify.com/docs/api/apify#apifycallactid-input-options) function.

Let's add a bit of code to the end of our actor to send this email:

```JavaScript
// ...
log.info('Starting the crawl.');
await crawler.run();
log.info('Crawl finished.');

log.info('Sending dataset link...');
const dataset = await Apify.openDataset();
const { id } = await dataset.getInfo();

await Apify.call('apify/send-mail', {
    to: 'YOUR_EMAIL@gmail.com',
    subject: 'Amazon Dataset',
    text: `Dataset link: https://api.apify.com/v2/datasets/${id}/items?clean=true&format=json`,
});

log.info('Finished.');
//...
```

> Refer to [this section](https://docs.apify.com/api/v2#/reference/datasets/dataset/get-dataset) of the API documentation to understand how we came up with this public dataset URL

`Apify.openDataset()` with no arguments passed to it opens up the default dataset, which we've been using. In future lessons, we'll be discussing the differences between named and unnamed storage.

## [](#pushing-to-the-platform) Pushing to the platform

You can push your actor to the platform by running `apify push` in terminal. Once it says it's completed, return back to the **Actors** section in your [Apify Console](https://console.apify.com?asrc=developers_portal). You should see that **demo-actor** is now apart of the list. The only problem is that when we click on it, it doesn't seem to support a **keyword** input! Instead, we see this:

![Forgot to configure input schema]({{@asset expert_scraping_with_apify/solutions/images/bad-schema.webp}})

This is problematic, because our actor is expecting an input value specifically with the key of `keyword`.

The reason this has happened is because we forgot to configure our [input schema](https://docs.apify.com/actors/development/input-schema). Heading back to our project, let's open up the **INPUT_SCHEMA.json** file and modify it to look like this:

```JSON
{
    "title": "Amazon Crawler",
    "description": "Crawl all the offers for the first page of results based on keyword.",
    "type": "object",
    "schemaVersion": 1,
    "properties": {
        "keyword": {
            "title": "Keyword",
            "type": "string",
            "description": "Enter a keyword for Amazon.",
            "editor": "textfield",
            "prefill": "iphone"
        }
    },
    "required": ["keyword"]
}
```

We've also replaced the default `title` and `description` with our own custom ones that describe what the actor does. Now, let's run `apify push` again to see these changes on the platform.

## [](#running-on-the-platform) Running the actor on the platform

Let's try it out now! Input **iphone** into the box labeled **keyword**, click **Start**, and see the sparks fly!

## [](#quiz-answers) Quiz answers ðŸ“

**Q: When using Puppeteer or Playwright, how can you still use jQuery with the SDK?**

**A:** There are two ways. You can either use the [injectJQuery](https://sdk.apify.com/docs/api/puppeteer#puppeteerinjectjquerypage) utility function which will enable you to use jQuery inside of `page.evalute()`, or you can use Cheerio to load the page's content like this:

```JavaScript
const $ = cheerio.load(await page.content());
```

Which will allow you to use jQuery syntax outside of Puppeteer/Playwright page evaluation functions.

**Q: What is the main difference between Cheerio and jQuery?**

**A:** Though they share the same syntax, and therefore look quite similar, Cheerio and jQuery are designed for different purposes. jQuery is a frontend library for manipulating the DOM, making AJAX calls, etc. while Cheerio is a backend library exclusively for Node.js built for the purpose of intuitively parsing HTML on the server-side.

Some specific functionality of jQuery is not available with Cheerio. Follow official documentation if you run into one of these cases.

**Q: In which situations would you use CheerioCrawler? What are its limitations?**

**A:** One should use CheerioCrawler when scraping any non-dynamic content. For scraping any content that doesn't require the loading of JavaScript in order to receive all of the data (such as with server-side rendered HTML pages and APIs), CheerioCrawler should be used. It is limited though, as it can only make static requests. This means that if a piece of data is loaded using JavaScript from an API call that the page makes, CheerioCrawler will never see that piece of data.

> Learn more about dynamic pages in our [**dynamic pages**]({{@link dynamic_pages_and_spas/js_rendering_and_dynamic_pages.md}}) lesson, and learn how to overcome their challenges in the [**API scraping**]({{@link api_scraping.md}}) course and the [**JSON in HTML**]({{@link dynamic_pages_and_spas/js_in_html.md}}) lesson.

Additionally, if the job being done requires some sort of interaction with the page, PlaywrightCrawler/PuppeteerCrawler should be used, as CheerioCrawler runs out of the context of the browser.

**Q: Using Puppeteer, how can you extract data from a page without using jQuery/Cheerio?**

**A:** You can use functions such as `page.evaluate()` and `page.$$eval()` to run data collection code in the context of the browser and return it so that it can be used back in the Node.js context.

Though this is possible:

```JavaScript
const title = await page.evaluate(() => document.querySelector('title').textContent);
```

It is still much more preferred to go with this option:

```JavaScript
const $ = cheerio.load(await page.content());
const title = $('title').text();
```

Or even this:

```JavaScript
await Apify.utils.puppeteer.injectJQuery(page);
const title = await page.evaluate(() => $('title').text());
```

**Q: What is the default concurrency the SDK uses? Why this number?**

**A:** The default value for `maxConcurrency` is set to **1000**, which allows the crawler to scale up automatically to this threshold. Concurrency in the SDK is managed by the `AutoscaledPool`, which uses snapshots of the environment the actor is running in to see if it can launch more tasks than are already running (AKA **scale up**).

**Q: What is the difference between the RequestList and the RequestQueue?**

The main differece is that once a request list has been created, no more requests can be dynamically added to it. When you want to dynamically add (or  remove) requests, a requst queue must be used.

Request lists are better when adding a large batch of requests, as the RequestQueue is not optimized to handle the mass adding of requests. Additionally, the RequestList doesn't consume any platform credits.

**Q: How can we send data between requests?**

**A:** We can use the `userData` key to pass data into a requests, access it when handling the request, then pass it along to another request through the `userData` key once again.

## [](#wrap-up) Wrap up

Whew, that was quite a large task! If you've made it up to this point, you're a total rockstar!
