---
title: Initializing & setting up
description: When you collect links from a web page, you often end up with a lot of irrelevant URLs. Learn how to filter the links to only keep the ones you need.
menuWeight: 1
paths:
- web-scraping-for-beginners/challenge/initializing-and-setting-up
---

# Initialization & setting up

The Crawlee CLI makes it extremely easy for us to set up a project in Crawlee and hit the ground running. Navigate to the directory you'd like your project's folder to live, then open up a terminal instance and run the following command:

```shell
npx crawlee create amazon-crawler

Once you run this command, you'll get prompted into a menu which you can navigate using your arrow keys. Each of these options will generate different boilerplate code when selected. We're going to work with CheerioCrawler today, so we'll select the **CheerioCrawler template project** template, then press **Enter**.

![Crawlee CLI "create" command]({{@asset web_scraping_for_beginners/challenge/images/crawlee-create.webp}})

Once it's completed, open up the **amazon-crawler** folder that was generated by the `npx crawlee create` command. We're going to modify the **main.js** boilerplate to fit our needs:

```JavaScript
// main.js
import { CheerioCrawler, KeyValueStore, log } from 'crawlee';
import { router } from './routes.js';

// Grab our keyword from the input
const { keyword = 'iphone' } = (await KeyValueStore.getInput()) ?? {};

const crawler = new CheerioCrawler({
    requestHandler: router,
});

// Add our initial requests
await crawler.addRequests([
    {
        // Turn the inputted keyword into a link we can make a request with
        url: `https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,
        label: 'START',
        userData: {
            keyword,
        },
    },
]);

log.info('Starting the crawl.');
await crawler.run();
log.info('Crawl finished.');
```

```JavaScript
// routes.js
import { createCheerioRouter } from 'crawlee';

export const router = createCheerioRouter();

router.addDefaultHandler(({ log }) => {
    log.info('Route reached.')
});
```

Finally, we'll modify our input file in **storage/key_value_stores/default/INPUT.json** to look like this:

```JSON
{
  "keyword": "iphone"
}
```

> This is how we'll be inputting data into our scraper from now on. Don't worry though, from now on, we'll only need  to work in the **main.js** and **routes.js** files!

## [](#next) Next up

Cool! We're ready. But [let's discuss a bit about modularity]({{@link web_scraping_for_beginners/challenge/modularity.md}}) before moving forward!
